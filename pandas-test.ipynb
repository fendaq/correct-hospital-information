{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "File b'test1.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-173421fede9a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test1.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#如果你的数据集中有中文的话，最好在里面加上 encoding = 'gbk' ，以避免乱码问题。后面的导出数据的时候也一样。\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#df.head(5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\anzhuang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    527\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 529\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\anzhuang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\anzhuang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    610\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 612\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    613\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_options_with_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\anzhuang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m    745\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    746\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\anzhuang\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1117\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'allow_leading_cols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1119\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;31m# XXX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas\\parser.c:3246)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas\\parser.c:6111)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File b'test1.csv' does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('test1.csv', header=0)\n",
    "#如果你的数据集中有中文的话，最好在里面加上 encoding = 'gbk' ，以避免乱码问题。后面的导出数据的时候也一样。\n",
    "#df会是一个dataframe\n",
    "df.head(5)\n",
    "df.tail(5)\n",
    "\n",
    "#重置列名\n",
    "df.columns = ['water_year','rain_octsep', 'outflow_octsep',\n",
    "              'rain_decfeb', 'outflow_decfeb', 'rain_junaug', 'outflow_junaug']\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "#共多少条记录\n",
    "len(df)\n",
    "\n",
    "\n",
    "pd.options.display.float_foemat = '{:,.3f}'.format\n",
    "df.describe()\n",
    "#这将返回一张表，其中有诸如总数、均值、标准差之类的统计数据：\n",
    "\n",
    "\n",
    "##################################################################################过滤\n",
    "df['rain_octsep']\n",
    "#提取一列值，是一个series,而不是dataframe\n",
    "df.rain_octsep\n",
    "\n",
    "\n",
    "df[df.rain_octsep < 1000]\n",
    "#使用条件过滤现有的dataframe\n",
    "\n",
    "df[(df.rain_octsep < 1000) & (df.outflow_octsep < 4000)] \n",
    "#这条代码只会返回 rain_octsep 中小于 1000 的和 outflow_octsep 中小于 4000 的记录：\n",
    "#注意重要的一点：这里不能用 and 关键字，因为会引发操作顺序的问题。必须用 & 和圆括号。\n",
    "\n",
    "\n",
    "df[df.water_year.str.startswith('199')]\n",
    "#必须用 .str.[string method] ，,使用字符串方法来进行过滤\n",
    "\n",
    "\n",
    "##################################################################################索引\n",
    "#如果你的行标签是数字型的，你可以通过 iloc 来引用：\n",
    "df.iloc[30]\n",
    "#iloc 只对数字型的标签有用。它会返回给定行的 series，行中的每一列都是返回 series 的一个元素。\n",
    "\n",
    "#设置新的索引\n",
    "df = df.set_index(['water_year'])\n",
    "\n",
    "df.head(5)\n",
    "\n",
    "#上例中我们设置的索引列中都是字符型数据，这意味着我们不能继续使用 iloc 来引用，那我们用什么呢？用 loc 。\n",
    "df.loc['2000/01']\n",
    "#和 iloc 一样， loc 会返回你引用的列，唯一一点不同就是此时你使用的是基于字符串的引用，而不是基于数字的。\n",
    "\n",
    "\n",
    "#如果 loc 是基于标签的，而 iloc 是基于数字的，那 ix 是基于什么的？\n",
    "#事实上， ix 是基于标签的查询方法，但它同时也支持数字型索引作为备选。\n",
    "df.ix['1999/00'] \n",
    "\n",
    "\n",
    "#利用索引排序。。。。。。。在 Pandas 中，我们可以对 dataframe 调用 sort_index 方法进行排序。\n",
    "df.sort_index(ascending=False).head(5) #降序号\n",
    "\n",
    "\n",
    "#*******当你将一列设置为索引的时候，它就不再是数据的一部分了。\n",
    "#果你想将索引恢复为数据，调用 set_index 相反的方法 reset_index 即可：\n",
    "df = df.reset_index('water_year')\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "##################################################################################对数据集应用函数\n",
    "#有时你想对数据集中的数据进行改变或者某种操作。比方说，你有一列年份的数据，你需要新的一列来表示这些年份对应的年代。\n",
    "#Pandas 中有两个非常有用的函数， apply 和 applymap 。\n",
    "# Applying a function to a column\n",
    "def base_year(year):\n",
    "    base_year = year[:4]\n",
    "    base_year= pd.to_datetime(base_year).year\n",
    "    return base_year\n",
    "\n",
    "df['year'] = df.water_year.apply(base_year)\n",
    "df.head(5)\n",
    "\n",
    "\n",
    "##################################################################################操作数据集的结构\n",
    "#另一常见的做法是重新建立数据结构，使得数据集呈现出一种更方便并且（或者）有用的形式。\n",
    "###挺乱的   之后要用到再细看\n",
    "\n",
    "\n",
    "\n",
    "##################################################################################合并数据集\n",
    "rain_jpn = pd.read_csv('jpn_rain.csv')\n",
    "rain_jpn.columns = ['year', 'jpn_rainfall']\n",
    "\n",
    "uk_jpn_rain = df.merge(rain_jpn, on='year')\n",
    "uk_jpn_rain.head(5)\n",
    "#首先你需要通过 on 关键字来指定需要合并的列。通常你可以省略这个参数，Pandas 将会自动选择要合并的列。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################使用 Pandas 快速作图\n",
    "#Matplotlib 很棒，但是想要绘制出还算不错的图表却要写不少代码，而有时你只是想粗略的做个图来探索下数据，搞清楚数据的含义。\n",
    "#Pandas 通过 plot 来解决这个问题：\n",
    "uk_jpn_rain.plot(x='year', y=['rain_octsep', 'jpn_rainfall'])\n",
    "#之前得到的的uk_jpn_rain是一个dataframe的形式，在此基础上使用plot\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################保存你的数据集\n",
    "#在清洗、重塑、探索完数据之后，你最后的数据集可能会发生很大改变，并且比最开始的时候更有用。\n",
    "#你应该保存原始的数据集，但是你同样应该保存处理之后的数据。\n",
    "\n",
    "\n",
    "###\n",
    "#http://www.open-open.com/lib/view/open1470724984453.html#articleHeader0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\anzhuang\\lib\\site-packages\\ipykernel\\__main__.py:95: FutureWarning: by argument to sort_index is deprecated, pls use .sort_values(by=...)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\ncount\\t非 NA 值的数量\\ndescribe\\t针对 Series 或 DF 的列计算汇总统计\\nmin , max\\t最小值和最大值\\nargmin , argmax\\t最小值和最大值的索引位置（整数）\\nidxmin , idxmax\\t最小值和最大值的索引值\\nquantile\\t样本分位数（0 到 1）\\nsum\\t求和\\nmean\\t均值\\nmedian\\t中位数\\nmad\\t根据均值计算平均绝对离差\\nvar\\t方差\\nstd\\t标准差\\nskew\\t样本值的偏度（三阶矩）\\nkurt\\t样本值的峰度（四阶矩）\\ncumsum\\t样本值的累计和\\ncummin , cummax\\t样本值的累计最大值和累计最小值\\ncumprod\\t样本值的累计积\\ndiff\\t计算一阶差分（对时间序列很有用）\\npct_change\\t计算百分数变化\\n'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pandas import Series,DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "####################################################################################Series\n",
    "#Series 可以看做一个定长的有序字典。基本任意的一维数据都可以用来构造 Series 对象：\n",
    "s = Series([1,2,3.0,'abc'])\n",
    "#s\n",
    "\n",
    "#Series 对象包含两个主要的属性：index 和 values\n",
    "s = Series(data=[1,3,5,7],index = ['a','b','x','y'])\n",
    "#print(s)\n",
    "#print(s.index)\n",
    "#print(s.values)\n",
    "\n",
    "#################################################################################Dataframe\n",
    "#DataFrame 是一个表格型的数据结构，它含有一组有序的列（类似于 index），\n",
    "#每列可以是不同的值类型（不像 ndarray 只能有一个 dtype）。\n",
    "#基本上可以把 DataFrame 看成是共享同一个 index 的 Series 的集合。\n",
    "data = {'state':['Ohino','Ohino','Ohino','Nevada','Nevada'],\n",
    "        'year':[2000,2001,2002,2001,2002],\n",
    "        'pop':[1.5,1.7,3.6,2.4,2.9]}\n",
    "df = DataFrame(data)\n",
    "#print(df)\n",
    "\n",
    "df = DataFrame(data,index=['one','two','three','four','five'],\n",
    "               columns=['year','state','pop','debt'])\n",
    "#print(df)\n",
    "\n",
    "\n",
    "\n",
    "#################################################################################对象属性----重新索引\n",
    "ser = Series([4.5,7.2,-5.3,3.6],index=['d','b','a','c'])\n",
    "a = ['a','b','c','d','e']\n",
    "ser.reindex(a)\n",
    "#print(ser.reindex(a))\n",
    "#print(ser.reindex(a,fill_value=0))\n",
    "                             ####出错：：print(ser.reindex(a,method='ffill'))\n",
    "#print(ser.reindex(a).fillna(method='ffill'))\n",
    "#.reindex() 方法会返回一个新对象，其 index 严格遵循给出的参数，\n",
    "#method:{'backfill', 'bfill', 'pad', 'ffill', None} 参数用于指定插值（填充）方式，\n",
    "#当没有给出时，自动用 fill_value 填充，默认为 NaN\n",
    "\n",
    "\n",
    "state = ['Texas','Utha','California']\n",
    "df.reindex(columns=state,method='ffill')\n",
    "#print(df.reindex(index=['a','b','c','d'],columns=state))\n",
    "\n",
    "\n",
    "###########################################################################删除\n",
    "#语法：   .drop(labels, axis=0)\n",
    "#print(ser)\n",
    "#print(df)\n",
    "#print(ser.drop('b'))\n",
    "#print(df.drop('two'))\n",
    "#*****************注意：.drop() 返回的是一个新对象，元对象不会被改变。***********************\n",
    "\n",
    "#print(df)\n",
    "#print(df.drop(['year','state'],axis=1))\n",
    "\n",
    "\n",
    "###########################################################################切片\n",
    "foo=Series(index=['a','b','c','d'],data=[4.5,7.2,-5.3,3.6])\n",
    "#print(foo)\n",
    "#print(foo[:2])\n",
    "#print(foo[:'c'])\n",
    "\n",
    "#DataFrame 对象的标准切片语法为：.ix[::,::]。\n",
    "#ix 对象可以接受两套切片，分别为行（axis=0）和列（axis=1）的方向：\n",
    "data={\n",
    "    'Ohio':[0,3,6],\n",
    "    'Texas':[1,4,7],\n",
    "    'California':[2,5,8]\n",
    "}\n",
    "df=DataFrame(data,index=['a','b','c'],columns=['Ohio','Texas','California'])\n",
    "#print(df)\n",
    "#print(df.ix[:2,:2])\n",
    "#print(df.ix['a','Ohio'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################算数运算和数据对齐\n",
    "foo=Series({'a':1 ,'b':2 })\n",
    "#print(foo)\n",
    "bar=Series({'b':3 ,'d':4 })\n",
    "#print(bar)\n",
    "#print(foo+bar)\n",
    "\n",
    "\n",
    "#   操作符：df1.add(df2,fill_value=0)。其他算术方法还有：sub(), div(), mul()。\n",
    "#   DataFrame 的对齐操作会同时发生在行和列上。\n",
    "\n",
    "#df.sort_index(by='Ohio')\n",
    "df1 = df.sort_index(by=['California','Texas'])\n",
    "#print(df1)\n",
    "\n",
    "#print(df.sort_index(axis=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##################################################################统计\n",
    "#df.mean()\n",
    "#df.mean(axis=1)\n",
    "#df.mean(axis=1,skipna=False)\n",
    "\n",
    "####其他统计方法\n",
    "'''\n",
    "count\t非 NA 值的数量\n",
    "describe\t针对 Series 或 DF 的列计算汇总统计\n",
    "min , max\t最小值和最大值\n",
    "argmin , argmax\t最小值和最大值的索引位置（整数）\n",
    "idxmin , idxmax\t最小值和最大值的索引值\n",
    "quantile\t样本分位数（0 到 1）\n",
    "sum\t求和\n",
    "mean\t均值\n",
    "median\t中位数\n",
    "mad\t根据均值计算平均绝对离差\n",
    "var\t方差\n",
    "std\t标准差\n",
    "skew\t样本值的偏度（三阶矩）\n",
    "kurt\t样本值的峰度（四阶矩）\n",
    "cumsum\t样本值的累计和\n",
    "cummin , cummax\t样本值的累计最大值和累计最小值\n",
    "cumprod\t样本值的累计积\n",
    "diff\t计算一阶差分（对时间序列很有用）\n",
    "pct_change\t计算百分数变化\n",
    "'''\n",
    "\n",
    "#   is(not)null\n",
    "#   这一对方法对对象做元素级应用，然后返回一个布尔型数组，一般可用于布尔型索引。\n",
    "#   处理 NA 的方法有四种：dropna , fillna , isnull , notnull 。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
